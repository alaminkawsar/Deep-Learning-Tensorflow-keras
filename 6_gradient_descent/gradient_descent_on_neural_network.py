# -*- coding: utf-8 -*-
"""gradient_descent_On_Neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/137apcVart8oMzIIDbt8BwtE9L34pG0xJ

#How Gradient Descent work in Neural Network#

with help of [codebasic](https://www.youtube.com/c/codebasics) tutorial
"""

# Commented out IPython magic to ensure Python compatibility.
#import neccessary library
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline

df = pd.read_csv("insurance_data.csv")
df.head()

"""Split train and test"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df[['age','affordibility']],df.bought_insurance,test_size=0.2, random_state=25)

"""**Preprocessing: Scale the data so that both age and affordibility are in same scaling range**"""

X_train_scaled = X_train.copy()
X_train_scaled['age'] = X_train_scaled['age']/100

X_test_scaled = X_test.copy()
X_test_scaled['age'] = X_test_scaled['age']/100

"""**Model Building**"""

model = keras.Sequential([
    keras.layers.Dense(1, input_shape=(2,), activation='sigmoid',kernel_initializer='ones')                      
])
model.compile(optimizer='adam',
              loss = 'binary_crossentropy',
              metrics=['accuracy'])
model.fit(X_train_scaled,y_train,epochs=5000)

"""**Evaluate the model**"""

model.evaluate(X_test_scaled,y_test)

model.predict(X_test_scaled)

y_test

"""**Now get the value of weights and bias from the model**


"""



coef, intercept = model.get_weights()

coef, intercept

"""**Above tensorflow model implement in python**"""

def sigmoid(x):
  import math
  return 1/(1+math.exp(-x))
sigmoid(20)

X_test

#Let's make predict function instead of tensorflow
def prediction_fun(age,affordibility):
  weighted_sum = coef[0]*age+coef[1]*affordibility+intercept
  return sigmoid(weighted_sum)
prediction_fun(.47, 1)

prediction_fun(.18, 1)

"""#Implementation gradient descent in plain python that is used in keras built in#

**seudo code:**


function parimeter
***gradient_descent(parameter_1, parameter_2,....,parameter_N, y_true, epochs, loss_thresold):***

*//initialization*
w_1=w_2=..=w_N=1, bias=0, learning_rate=0.5

    1. weighted_sum = w_1*parameter_1+w_2*paremeter_2+...+wN*paremeterN+bias
    2. y_predict=sigmoid(weighted_sum)
    3. loss = log_loss(y_true,y_predicted)
      //need to implement log_loss or binary_crossentropy
    4. WDif_i = (1/n)*(transpose_matrix(parameter_i)*(y_predicted-y_true))
      where i=1,2,3,...,N
    5. w_i = w_i - learning_rate*WDif_i
      where i=1,2,3,...,N
    6. BIASDif = Mean(y_predicted - y_true)
    7. bias = bias - learning_rate*BIASDif
    8. if loss<=loss_thresold:
      END Program
    9. continue the process 1 to 9 with 0 to epochs value.
    10. END Program


**sigmoid function**
*function parameter: sigmoid(z)*

    1. 1/(1+e^(-z)
      where z = predicted value

**binary_cossentropy/log_loss**
*function parameter:   log_loss(y_true, y_predicted)*

*math: binary_crossentropy = (-1/n) x SUM(y_i x log(yP_i)+(1-y_i) x log(1-yP_i))*

    1. epsilon=1e-15
    2. y_predicted_new = [max(i,epsilon) for i in y_predicted]
    3. y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
    4. mean = Mean(y_true*log(y_predicted_new)+(1-y_true)*log(1-y_predicted_new)
    5. return mean

Let's Dive into Code for two above problem
"""

#sigmoid function
def sigmoid_numpy(X):
   return 1/(1+np.exp(-X))

sigmoid_numpy(10)

#log_loss or binary_crossentropy
def log_loss(y_true, y_predicted):
    epsilon = 1e-15
    y_predicted_new = [max(i,epsilon) for i in y_predicted]
    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]
    y_predicted_new = np.array(y_predicted_new)
    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))

def gradient_descent(age, affordability, y_true, epochs, loss_thresold):
    w1 = w2 = 1
    bias = 0
    rate = 0.5
    n = len(age)
    for i in range(epochs):
        weighted_sum = w1 * age + w2 * affordability + bias
        y_predicted = sigmoid_numpy(weighted_sum)
        loss = log_loss(y_true, y_predicted)

        w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true)) 
        w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true)) 

        bias_d = np.mean(y_predicted-y_true)
        w1 = w1 - rate * w1d
        w2 = w2 - rate * w2d
        bias = bias - rate * bias_d

        print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')

        if loss<=loss_thresold:
            break

    return w1, w2, bias

#no run the model
gradient_descent(X_train_scaled['age'],X_train_scaled['affordibility'],y_train,1000, 0.4631)

coef, intercept

